{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhammadolammi/llms-from-scratch/blob/main/gpt_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "70m8KBUQmkFN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "import pickle\n",
        "import mmap\n",
        "import random\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o2v7jS-zwcIL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a0ac6ef-1755-4464-857b-243c8c74b25d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BLOCK_SIZE =64\n",
        "BATCH_SIZE = 128\n",
        "MAX_ITER = 500\n",
        "LEARNING_RATE = 3e-4\n",
        "EVAL_ITERS = 50\n",
        "N_EMBD = 384\n",
        "N_LAYERS = 8\n",
        "N_HEAD = 8\n",
        "DROP_OUT =0.2\n",
        "MODEL_PKL_DIR =\"drive/MyDrive/GPT/models/model01.pkl\"\n",
        "DATA_fOLDER=\"/content/drive/MyDrive/Colab Datasets/openwebtext/openwebtext\"\n",
        "\n",
        "# book_dir = \"drive/MyDrive/Colab Datasets/wizard of oz.txt\"\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars =\"\"\n",
        "with open(f\"{DATA_fOLDER}/vocab.txt\", \"r\" ) as f:\n",
        "  text = f.read()\n",
        "  chars = sorted(list(set(text)))"
      ],
      "metadata": {
        "id": "x01X0lcPByKg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(set(chars + ['[UNK]']))\n",
        "\n",
        "vocab_size = len(chars)\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMOuzkgRe5Lx",
        "outputId": "1590951a-fdfe-464c-c847-7ea0911db26e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5524"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
        "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [string_to_int.get(c, string_to_int['[UNK]']) for c in s]\n",
        "decode = lambda l: ''.join([int_to_string[i] for i in l])\n"
      ],
      "metadata": {
        "id": "R6K0wUXYgOcx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QzO_yOUTy-Zu"
      },
      "outputs": [],
      "source": [
        "# memory map for using small snippets of text from a single file of any size\n",
        "def get_random_chunk(split, ):\n",
        "\n",
        "    filename = os.path.join(DATA_fOLDER, \"train_split.txt\") if split == 'train' else os.path.join(DATA_fOLDER, \"val_split.txt\")\n",
        "    with open(filename, 'rb') as f:\n",
        "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
        "            # Determine the file size and a random position to start reading\n",
        "            file_size = len(mm)\n",
        "            start_pos = random.randint(0, (file_size) - BLOCK_SIZE*BATCH_SIZE)\n",
        "\n",
        "            # Seek to the random position and read the block of text\n",
        "            mm.seek(start_pos)\n",
        "            block = mm.read(BLOCK_SIZE*BATCH_SIZE-1)\n",
        "\n",
        "            # Decode the block to a string, ignoring any invalid byte sequences\n",
        "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
        "\n",
        "            # Train and test splits\n",
        "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    split = split.lower()\n",
        "    data = get_random_chunk(split)\n",
        "    ix = torch.randint(len(data)- BLOCK_SIZE, (BATCH_SIZE,))\n",
        "    # print(ix)\n",
        "    # to get our batch we just stack bocks on each other\n",
        "    x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix] )\n",
        "    y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix] )\n",
        "    x,y = x.to(device), y.to(device)\n",
        "    return x,y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "USx7gENmy__j"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self,head_size):\n",
        "     super().__init__()\n",
        "     # create k, q, v using linear transformation\n",
        "     self.key = nn.Linear(N_EMBD, head_size, bias=False)\n",
        "     self.query = nn.Linear(N_EMBD, head_size, bias=False)\n",
        "     self.value = nn.Linear(N_EMBD, head_size, bias=False)\n",
        "     self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
        "\n",
        "     self.dropout = nn.Dropout(DROP_OUT)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "\n",
        "    # input  size (batch, time-step, channels)\n",
        "    # output  size (batch, time-step, head size)\n",
        "    B,T,C = x.shape\n",
        "\n",
        "    # get k and q\n",
        "    k = self.key(x) # (B,T,hs)\n",
        "    q = self.query(x) # (B,T,hs)\n",
        "\n",
        "    # calculating attention scores using dot scaled product\n",
        "    #we transpose k and exchange index 1,2\n",
        "    wei = q @ k.transpose(-2,-1)  # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "    # scale by 1/sqrt(len of k/q rows)\n",
        "    wei =  wei /  k.shape[-1] ** 0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "    wei = self.dropout(wei)\n",
        "    # perform the weighted aggregation of the values\n",
        "    v = self.value(x) # (B,T,hs)\n",
        "    out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "    return out # (B, T, hs)\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads ):\n",
        "     super().__init__()\n",
        "     self.heads = nn.ModuleList( [ Head(num_heads) for _ in range(N_HEAD)])\n",
        "     # projection is the linear transformation in the multihead\n",
        "     self.proj = nn.Linear(num_heads*N_HEAD, N_EMBD)\n",
        "     ## add a dropout\n",
        "     self.dropout = nn.Dropout(DROP_OUT)\n",
        "  def forward(self, x):\n",
        "    ## concat n-head dotproduct attention results by features i.e concat them in the feature dimension\n",
        "    out = torch.concat([head(x) for head in self.heads], dim=-1)\n",
        "    ## add linear\n",
        "    out = self.proj(out)\n",
        "    ## dropout\n",
        "    out = self.dropout(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, ):\n",
        "     super().__init__()\n",
        "     self.net = nn.Sequential(\n",
        "         nn.Linear(N_EMBD, 4* N_EMBD),\n",
        "         nn.ReLU(),\n",
        "         nn.Linear(4*N_EMBD,  N_EMBD),\n",
        "         nn.Dropout(DROP_OUT)\n",
        "\n",
        "\n",
        "     )\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "\n",
        "class LAYER(nn.Module):\n",
        "  def __init__(self):\n",
        "     super().__init__()\n",
        "     self.ff = FeedForward()\n",
        "     head_size = N_EMBD // N_HEAD\n",
        "     self.sa = MultiHeadAttention(head_size)\n",
        "     self.ln1 = nn.LayerNorm(N_EMBD)\n",
        "     self.ln2 = nn.LayerNorm(N_EMBD)\n",
        "  def forward(self, x):\n",
        "    ## x1 is the output from self attention\n",
        "    x1 = self.sa(x)\n",
        "    # we do a residual connection here, by adding initial x and  the result from sa then normalizing\n",
        "    x = self.ln1(x1+x)\n",
        "    #x1 is the output from feed forwad\n",
        "    x1 = self.ff(x)\n",
        "    # we do a residual connection here, by adding initial x and  the result from feed forward then normalizing\n",
        "    x = self.ln1(x1+x)\n",
        "    return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, N_EMBD)\n",
        "        self.position_embedding_table = nn.Embedding(BLOCK_SIZE, N_EMBD)\n",
        "        ## create the sequential decoder layers\n",
        "        self.layers = nn.Sequential(*[LAYER() for _ in range(N_LAYERS)])\n",
        "\n",
        "        # final layer norm function for add & norm\n",
        "        self.ln_f = nn.LayerNorm(N_EMBD)\n",
        "\n",
        "        # linear head\n",
        "        self.lm_head = nn.Linear(N_EMBD, vocab_size)\n",
        "\n",
        "        # initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, index, targets=None):\n",
        "        # forward function, should return logits and the loss of current layer/run\n",
        "        # instead of getting logit randomly using embedding(As we did with Bigram)\n",
        "        # now we can use our transformers\n",
        "        # logits = self.token_enbedding_table(index)\n",
        "        # B = BATCH SIZE\n",
        "        # T = BLOCK SIZE\n",
        "        #C=N_EMBD\n",
        "        B, T = index.shape\n",
        "        # embed input\n",
        "        tok_embd = self.token_embedding_table(index) # (B,T,C)\n",
        "        # get pos embeding/encoding for\n",
        "        positions = torch.arange(T, device=device)\n",
        "        position_embd = self.position_embedding_table(positions) # (T, C)\n",
        "        # add pos embd and tokenembd\n",
        "        # we can add since # (B,T,C)   + (T, C) is the broadcast rule cause (T, C) is just treated as (1, T, C)\n",
        "        x = tok_embd +  position_embd # (B,T,C)\n",
        "        # send x to decode layers\n",
        "        x = self.layers(x) # (B,T,C)\n",
        "        # add the layer norm\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        #get logits with linear  head\n",
        "        logits = self.lm_head(x) # (B,T,C)\n",
        "\n",
        "        if targets==None:\n",
        "          ## if no target, just return logit and None for loss\n",
        "          return logits, None\n",
        "        else:\n",
        "\n",
        "          # WE need to reshape logit to use the cross_entropy loss function\n",
        "\n",
        "          B, T, C = logits.shape\n",
        "          # loss = F.cross_entropy(logit.view(B*T, C), targets.view(B*T))\n",
        "          #we can actually run this ^^ fine and logit will still be 3 dimensional, according to the tutorila\n",
        "          # when theres a target logit should return 2 dimension , so will run these VV for now\n",
        "          logits = logits.view(B*T, C)\n",
        "          loss = F.cross_entropy(logits, targets.view(B*T))\n",
        "          return logits, loss\n",
        "\n",
        "    # generate generate tokens.\n",
        "    def generate(self, index, max_new_tokens):\n",
        "      # index is (B,T) array of index in current context\n",
        "      for _ in range(max_new_tokens):\n",
        "        # crop idx to the last block_size tokens\n",
        "        index_cond = index[:, -BLOCK_SIZE:]\n",
        "        # generate predictions\n",
        "\n",
        "        logits, loss = self.forward(index_cond)\n",
        "        # target is none here, so logits is in (B, T, C)\n",
        "        # get logits for last T\n",
        "\n",
        "        logits = logits[:,-1,:] # becomes (B, C)\n",
        "\n",
        "        # get probabilities for these logits/predictions using softmax\n",
        "        probs = F.softmax(logits, dim=-1) # return (B,C)\n",
        "        # sample from the probabilities distrubution\n",
        "        index_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        # add sampled index to running sequence\n",
        "        index = torch.cat((index, index_next), dim=1) # becomes (B , T+1)\n",
        "      # return the final index\n",
        "      return index\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uLVgjOagHs0U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4915600-8e37-4625-cdac-a5ba35b81ca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading model parameters\n",
            "loaded succesfully.\n"
          ]
        }
      ],
      "source": [
        "model = GPTLanguageModel()\n",
        "print(\"loading model parameters\")\n",
        "with open(MODEL_PKL_DIR, \"rb\") as f:\n",
        "  model = pickle.load(f)\n",
        "print(\"loaded succesfully.\")\n",
        "\n",
        "m = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HUnfX3wzfALI"
      },
      "outputs": [],
      "source": [
        "## loss report\n",
        "\n",
        "@torch.no_grad()\n",
        "\n",
        "def estimate_loss():\n",
        "  out ={}\n",
        "  model.eval()\n",
        "  for split in [\"train\", \"val\"]:\n",
        "    losses = torch.zeros(EVAL_ITERS)\n",
        "    for k in range(EVAL_ITERS):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss  = model(X,Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yUWAF-eDfBXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83e03390-e4c8-4c8b-f3a0-a662c850a7bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "steps: 0, train loss: 1.5028, val loss: 1.5421\n",
            "steps: 50, train loss: 1.5152, val loss: 1.5954\n",
            "steps: 100, train loss: 1.5871, val loss: 1.5194\n",
            "steps: 150, train loss: 1.5280, val loss: 1.4794\n",
            "steps: 200, train loss: 1.5384, val loss: 1.5552\n",
            "steps: 250, train loss: 1.4847, val loss: 1.5017\n",
            "steps: 300, train loss: 1.4677, val loss: 1.5049\n",
            "steps: 350, train loss: 1.4766, val loss: 1.5025\n",
            "steps: 400, train loss: 1.5088, val loss: 1.5332\n",
            "steps: 450, train loss: 1.5212, val loss: 1.5355\n",
            "1.3842111825942993\n"
          ]
        }
      ],
      "source": [
        "## optimizer\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=LEARNING_RATE )\n",
        "\n",
        "for iter in range(MAX_ITER):\n",
        "  if iter % EVAL_ITERS ==0:\n",
        "    # evaluate here\n",
        "    losses = estimate_loss()\n",
        "    print(f\"steps: {iter}, train loss: {losses['train']:.4f}, val loss: {losses['val']:.4f}\")\n",
        "\n",
        "  # optimize\n",
        "\n",
        "\n",
        "  # get batch of train data\n",
        "  xb, yb = get_batch(\"train\")\n",
        "\n",
        "  # evaluate the loss\n",
        "  logits, loss = model.forward(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "print(loss.item())\n",
        "\n",
        "\n",
        "with open(MODEL_PKL_DIR, \"wb\") as f:\n",
        "  pickle.dump(model,f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BkH2K6rn7nyq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "891ec62f-6020-457c-f50e-e1b1ca042d2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! Can you see me? b.\" Barie sqatroo rizeemmonistriboutside careinranty Fluender 20000 and BE caling Helwanlay, Selour\n"
          ]
        }
      ],
      "source": [
        "prompt = 'Hello! Can you see me?'\n",
        "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
        "generated_chars = decode(m.generate(context.unsqueeze(0), max_new_tokens=100)[0].tolist())\n",
        "print(generated_chars)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Fb61oWSzsjPwAIW-MJTpPq7UHUXCLX04",
      "authorship_tag": "ABX9TyOloCef8SIbfq6fbD3jkUOy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}